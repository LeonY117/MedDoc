{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/l.yao/miniconda3/envs/meddoc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pathlib\n",
    "from typing import List\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain.text_splitter import MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "\n",
    "import pymupdf4llm\n",
    "\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "\n",
    "from langchain_unstructured import UnstructuredLoader\n",
    "\n",
    "from unstructured.chunking.basic import chunk_elements\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "from langchain_chroma import Chroma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path = \"/Users/l.yao/Documents/MedDoc/local/Leave-Policy-Chapter-2-Annual-Leave-Procedure-V4-Rev-Jan2025.pdf\"\n",
    "pdf_path = \"/Users/l.yao/Documents/MedDoc/local/Leave-Policy-Framework-W19-PAG-Nov24-Final-v3.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader = UnstructuredLoader(\n",
    "#     file_path=[pdf_path],\n",
    "#     strategy=\"auto\",\n",
    "#     partition_via_api=False\n",
    "# )\n",
    "# docs = loader.load()\n",
    "# \n",
    "# [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = partition_pdf(filename=pdf_path, strategy=\"hi_res\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocr_elements = partition_pdf(filename=pdf_path, strategy=\"ocr_only\")\n",
    "# fast_elements = partition_pdf(filename=pdf_path, strategy=\"fast\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunk_by_title(\n",
    "    elements,\n",
    "    max_characters=1000,\n",
    "    combine_text_under_n_chars=500,\n",
    "    multipage_sections=False,\n",
    "    overlap=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks[0].metadata.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse chunks into texts\n",
    "\n",
    "texts = []\n",
    "metadatas = []\n",
    "\n",
    "for chunk in chunks:\n",
    "    text = chunk.text\n",
    "    metadata = chunk.metadata.to_dict()\n",
    "    metadata = {\n",
    "        \"filename\": metadata[\"filename\"],\n",
    "        \"page_number\": metadata[\"page_number\"],\n",
    "    }\n",
    "    texts.append(text)\n",
    "    metadatas.append(metadata)\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "    texts=texts,\n",
    "    metadatas=metadatas,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=CHROMA_PATH  # Optional: folder to save the DB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = db.similarity_search(\"How many days in advance to take adoption leave?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(doc.metadata)\n",
    "    print(\"\\n\\n\" + \"-\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import textwrap\n",
    "import os\n",
    "\n",
    "from langchain_core.messages.utils import count_tokens_approximately\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"/Users/l.yao/Documents/MedDoc/backend/ingestion/chroma_db\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embeddings)\n",
    "question = \"How many days in advance to ask for adoption leave?\"\n",
    "\n",
    "docs = db.similarity_search(question, k=3)\n",
    "\n",
    "context = '\\n\\n'.join(f\"{doc.page_content} + \\n {doc.metadata}\" for doc in docs)\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are an HR assistant for hospital staff. \"\n",
    "    \"Answer the user question based solely on the provided HR policy context. \"\n",
    "    \"If the context does not contain the answer, reply that you do not know instead of inventing one.\"\n",
    "    \"When you have the answer, please state the answer by adhering closely to the original text, and do not add any additional information.\"\n",
    "    \"At the end of your answer, cite the page number and document name of the text that you used to answer the question.\"\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=system_prompt),\n",
    "    HumanMessage(content=textwrap.dedent(f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {question}\n",
    "    \"\"\")),\n",
    "]\n",
    "\n",
    "print(count_tokens_approximately(messages))\n",
    "# calculate how many tokens are in the query\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "model = init_chat_model(\n",
    "    OPENAI_MODEL,\n",
    "    model_provider=\"openai\",\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=OPENAI_MODEL,\n",
    "    messages=messages,\n",
    "    max_tokens=None,\n",
    ")\n",
    "\n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Old test (didn't work very well).\n",
    "\n",
    "doc = fitz.open(pdf_path)\n",
    "md_text = pymupdf4llm.to_markdown(doc)\n",
    "\n",
    "# print(md_text)\n",
    "\n",
    "splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[(\"*\", \"Header 1\"), (\"**\", \"Header 2\")])\n",
    "\n",
    "split_text = splitter.split_text(md_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meddoc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
